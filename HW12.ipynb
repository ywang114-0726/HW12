{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# HW12\n",
    "\n",
    "## Neural Networks for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from os import listdir, path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_utils import PCA, StandardScaler\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from data_utils import LFWUtils\n",
    "\n",
    "from image_utils import make_image, open_image\n",
    "\n",
    "from HW12_utils import HW12Utils, FaceDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and PCA\n",
    "\n",
    "We started seeing in class how Neural Networks can be _easy_ to build and explain in generic/abstract terms (a bunch of little operators that perform weighted sums of their inputs), but in reality can be really difficult and opaque to steer.\n",
    "\n",
    "In theory, a couple of well placed neuron layers, with the right hyperparameters, learning rate, loss function, architecture and a good amount of data, can learn to extract information like polynomial features, clusters or PCA components. But... that's not always the case and sometimes it's not a bad idea to push/bias/encourage the network to go down a certain path.\n",
    "\n",
    "One way to do this is to pre-process our inputs and do a bit of feature extraction ourselves.\n",
    "\n",
    "Let's see if we can improve the face classification network from class by adding PCA information to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with the Data\n",
    "\n",
    "As always, we start with the data, and this time put the pixel and label information straight into `Tensor` objects, but only so we can create a `DataLoader` for each of our datasets.\n",
    "\n",
    "As a refresher... a `DataLoader` is a `PyTorch` objects that helps us _schedule_ how/when our data samples go through the model, by doing $2$ things, _batching_ and _shuffling_.\n",
    "\n",
    "Batching is the process of selecting portions of our dataset, and shuffling is the process of switching the order of the samples before they go into batches.\n",
    "\n",
    "There's usually no reason for shuffling the `test` dataset, but it's good to change up the order of our samples during training so the network doesn't learn just the order of the correct answers during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "x_train = Tensor(train[\"pixels\"])\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(test[\"pixels\"])\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FaceDataset` class we saw in class is available in the `HW12_utils` file, so we can just use it here to wrap our `Tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(FaceDataset(x_train, y_train), batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(FaceDataset(x_test, y_test), batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train !\n",
    "\n",
    "Oh no ! We don't have access to a GPU, so we can't use big neural networks.\n",
    "\n",
    "Also, our dataset is kind of small, and only has at most $15$ samples from each class...\n",
    "\n",
    "In these situations, it's not a bad idea to pre-process our data before sending it through training.\n",
    "\n",
    "Let's create a single layer neural network, like the one from class, and train it with the `train_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create the model, optimizer and loss function objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loop\n",
    "\n",
    "Create a training loop like we saw in class.\n",
    "\n",
    "The inner part of this loop should:\n",
    "- Iterate through all batches of our `DataLoader`\n",
    "- For each batch, predict classes\n",
    "- Measure `loss`\n",
    "- Get the optimizer to compute gradients\n",
    "- Update parameters\n",
    "\n",
    "This inner loop should be repeated as long as the loss keeps improving, and it doesn't look like the model is overfitting with the training data.\n",
    "\n",
    "In order to check if the model is overfitting, we should sporadically run evaluation within the training loop in order to see if the model performs similarly with `train` and `test` data.\n",
    "\n",
    "We can use the `HW12Utils.get_labels(model, dataloader)` to run our `model` on all the data in a given `dataloader` and return the true labels and predicted labels for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: iterate epochs\n",
    "  # TODO: iterate batches\n",
    "    # TODO: predict\n",
    "    # TODO: measure loss\n",
    "    # TODO: compute gradient and step optimizer\n",
    "    # TODO: show progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "This should be similar to the last error values seen during training, but sometimes it changes a bit...\n",
    "\n",
    "Not a bad idea to check the accuracy of the model using the `classification_error()` function, and look at some confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: classification error for train and test data\n",
    "# TODO: confusion matrices for train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PCA\n",
    "\n",
    "We're going to repeat the training, but this time our data is gonna be scaled and PCA'd before going into the neural network.\n",
    "\n",
    "Although it would be a bit unconventional for other types of models to scale `PCA` results, since our networks work better with scaled, well-proportioned data, here we'll have to scale the `PCA` results as well.\n",
    "\n",
    "So, the data preparation flow should be:\n",
    "- Scale data for `PCA`\n",
    "- Perform `PCA`\n",
    "- Scale data for Network\n",
    "\n",
    "We need two `StandardScaler()` objects and one `PCA()` object.\n",
    "\n",
    "The `train` data goes through the `fit_transform()` function of these objects, while the `test` data only goes through `transform()`.\n",
    "\n",
    "For the `PCA`, an explained variance of about $90\\%$ should be good, and, after the whole flow is set up, is something that can be experimented with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Scale\n",
    "# TODO: PCA\n",
    "# TODO: Scale\n",
    "# TODO: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "\n",
    "Re-create `DataLoaders`, model, optimizer, loss function, then re-run the training loop and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: DataLoaders\n",
    "# TODO: Model, Optimizer and Loss Function\n",
    "# TODO: Training loop\n",
    "# TODO: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "So... What happened ?<br>How does the <code>PCA</code> version of the NN compare to the regular version ?<br>How much of the improvement is from <code>PCA</code> and how much from scaling ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('hf-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
